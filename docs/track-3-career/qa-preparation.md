---
title: "Q&A Preparation"
sidebar_position: 11
---

# Q&A Preparation: Strategic Response Guide

**Stanford CREATE+AI Finals | 4-Minute Q&A Session**

This document provides prepared responses aligned with the evaluation criteria and judge expertise areas. Practice these until natural.

---

## Equity & Access Questions

### "How does this specifically serve underserved or marginalized learners?"

> "By design, not by hope. Three mechanisms:
>
> First, **barrier-free entry**—no coding prerequisites, no GPA requirements, no equipment costs. Selection based on interest and commitment, not background.
>
> Second, **explicit parity targets** we track and report: 50% female, 40% free/reduced lunch, 30% first-generation college. If disparities emerge, we adjust program design.
>
> Third, **portfolios as equalizers**. Research shows portfolio-based hiring reduces demographic bias compared to credential-based evaluation. Students from any background demonstrate capability through work samples, not pedigree signals.
>
> We're not hoping diversity happens—we're designing systems where it's built in."

### "What happens in under-resourced schools without reliable internet or devices?"

> "We've designed for variable access from day one.
>
> All equipment is **provided during program sessions**—no student needs their own device. The platform is designed for **low-bandwidth environments** with offline-capable components for core learning activities.
>
> El Segundo gives us a controlled pilot environment, but our Year 2 expansion specifically targets districts with varying infrastructure. We're building the technical architecture now to handle intermittent connectivity—that's a feature requirement, not an afterthought.
>
> The bigger equity issue isn't just device access—it's **network access** to professionals and opportunities. That's what we're really solving."

### "Who gets left out if this fails?"

> "The same students who are already being left behind by traditional pathways—first-generation students without professional networks, students from under-resourced communities without exposure to tech careers, students whose capabilities don't show up in GPA or test scores.
>
> That's why we're obsessive about measurement. We track outcomes disaggregated by demographics. If any subgroup isn't succeeding, that's a signal to redesign, not to accept.
>
> Failure for us isn't just 'program doesn't work'—it's 'program works for some students but not others.' We won't accept that."

---

## AI & Learning Science Questions

### "What evidence do you have that this improves learning outcomes?"

> "We're building on proven foundations with new measurement.
>
> The **evidence base** is strong: Schmidt and Hunter's 85-year meta-analysis shows work samples predict job performance at r=.54 versus r=.10 for credentials. Lave and Wenger's research proves authentic context produces deeper skill transfer. Topping's work demonstrates near-peer mentorship accelerates learning.
>
> What's new is applying these to AI collaboration skills. Our Year 1 pilot will generate the **first rigorous data** on portfolio-based AI skill development. We're partnering with researchers to measure not just placement outcomes but skill transfer and retention.
>
> We're not claiming proven outcomes yet—we're claiming a research-backed approach with a rigorous measurement plan."

### "How do you address AI sycophancy—LLMs agreeing with users rather than providing honest feedback?"

> "That's exactly why we designed the AI as a **Socratic coach**, not an answer machine.
>
> Our AI is specifically prompted to ask probing questions, not validate assumptions. When a student proposes a solution, the AI doesn't say 'great idea!'—it asks 'what evidence supports that?' and 'what would need to be true for this to fail?'
>
> More importantly, students learn to **expect challenge**. We explicitly teach that if the AI is only agreeing with you, you're not learning. The verification mindset is core curriculum, not optional.
>
> And ultimately, **employer reviewers** provide the honest feedback that matters most. AI can be sycophantic; professionals evaluating your portfolio won't be."

### "What happens when students become dependent on AI assistance?"

> "That's the crutch problem we're solving—teaching students to fish, not giving them fish.
>
> Our scaffolding **intentionally fades**. Early phases provide more AI support; later phases require students to work with less. By the employer project phase, students must demonstrate they can decompose problems, research solutions, and execute—with AI as one tool among many.
>
> We also explicitly teach **AI discernment**: recognizing when AI is helpful versus when it's a crutch, identifying hallucinations, knowing when human judgment must override AI suggestions.
>
> The goal isn't AI proficiency—it's **metacognitive capability** that transfers across any tool."

### "What learning theory underlies your approach?"

> "Three converging frameworks:
>
> **Situated learning theory** from Lave and Wenger—learning happens best in authentic contexts with real stakes and real audiences. That's why we use employer projects, not simulations.
>
> **Socio-cultural learning theory**—knowledge is constructed through social interaction. That's why near-peer mentorship is central, not optional.
>
> **Epistemic fluency** from Markauskaite and Goodyear—professional capability requires not just knowledge but the ability to evaluate and create knowledge. That's why we teach students to verify AI outputs and develop independent judgment.
>
> These aren't trendy frameworks—they're decades of validated research we're applying to a new challenge."

### "What proven pedagogy is this based on? How will you scale it?"

> "We're scaling **what already works**—portfolio assessment, near-peer mentorship, project-based learning in authentic contexts. These are proven pedagogies that traditional education has struggled to scale due to resource constraints.
>
> AI changes that equation. The Socratic AI coach enables personalized guidance at scale. The platform enables consistent project frameworks across locations. Employer review panels can evaluate portfolios remotely.
>
> We're not inventing new pedagogy—we're using AI to **remove the bottlenecks** that have prevented proven approaches from reaching more students."

---

## Teacher & Human-Centered Questions

### "How does this support teachers rather than replace them?"

> "Teachers are essential—AI amplifies their impact, doesn't substitute for it.
>
> The AI coach handles **scaffolded guidance** through problem decomposition—the repetitive explanations that consume teacher time. This frees teachers to focus on what humans do best: **relationship building**, motivational support, and navigating complex interpersonal dynamics.
>
> Teachers also gain **real-time visibility** into student progress through the platform. They see where students are struggling before those struggles become failures.
>
> We're not automating teaching—we're automating the parts of instruction that prevent teachers from teaching."

### "How did you involve actual learners and teachers in the design process?"

> "El Segundo district partnership isn't just implementation—it's co-design.
>
> We've conducted **listening sessions** with district teachers about their current challenges with career preparation. Students from the target demographic reviewed early platform prototypes and gave direct feedback on what resonates.
>
> The employer council includes professionals who will actually review student portfolios—their input shapes what 'job-ready' means in practice.
>
> This isn't a solution built in a lab and deployed to schools. It's built **with** the people who will use it."

---

## Business & Sustainability Questions

### "Walk me through the unit economics. What's the business model at scale?"

> "Year 1 cost is $521 per student—competitive with existing CTE programs that often run $1,500-3,000 per student.
>
> **Three revenue streams** for sustainability:
>
> 1. **District CTE funding**—schools already allocate budgets for career-technical education. We're a better use of those dollars.
>
> 2. **Employer sponsorships**—companies pay to access the talent pipeline and shape curriculum relevant to their needs. Boeing and Northrop are already engaged.
>
> 3. **Platform licensing**—Year 3 SaaS model lets other districts adopt the system with their own local employer networks.
>
> The $50,000 from Stanford proves the model. District contracts and employer partners cover ongoing costs."

### "What's the ROI for all stakeholders—employers, learners, institutions?"

> "**For learners**: Paid micro-internships, portfolio of validated work, direct employer relationships—versus unpaid internships they can't access and credentials that don't prove capability.
>
> **For employers**: Pre-screened candidates with demonstrated skills, reduced hiring risk, talent pipeline from local community—versus expensive recruiting with high early-career turnover.
>
> **For institutions**: CTE pathway that actually leads to employment, measurable outcomes for accreditation, differentiated program offering—versus career prep that doesn't connect to careers.
>
> Everyone wins because we're solving a real coordination problem, not just adding another program."

### "Is AI a feature or a platform for your product?"

> "AI is the **enabling infrastructure**, but the value is the **outcome system**.
>
> The Socratic AI coach is essential—it's how we provide personalized scaffolding at scale. But students don't come for the AI; they come for **portfolios that get them hired**.
>
> If better AI models emerge, we integrate them. If AI capabilities change, we adapt. The durable value is the employer network, the assessment methodology, and the proven pathway from learning to employment.
>
> We're not an AI company—we're a career pathway company that uses AI strategically."

### "How do you differentiate from incumbents who are rapidly adding AI?"

> "Incumbents are adding AI to **existing credential-based models**—AI tutoring, AI grading, AI course recommendations. They're making the old system more efficient.
>
> We're building a **different system entirely**—portfolio-based, employer-validated, focused on capability proof rather than credential accumulation.
>
> LinkedIn Learning can add AI features. Coursera can add AI tutors. But they can't easily rebuild their entire value proposition around employer-validated portfolios. That would cannibalize their credential-selling business.
>
> Our competition isn't other edtech—it's the status quo of credentials that don't prove capability."

### "How will you build distribution faster than incumbents can innovate?"

> "We're not competing on distribution breadth—we're competing on **depth of employer integration**.
>
> El Segundo gives us proof of concept with real employer outcomes. That proof enables conversations with districts who want **demonstrated results**, not promises.
>
> Incumbents have broad distribution but shallow employer relationships. We're building deep relationships first, then expanding to districts that want what we've proven works.
>
> The question isn't 'can we reach more students than Coursera?' It's 'can we prove a better pathway?' That's what Year 1 accomplishes."

---

## Team & Execution Questions

### "What makes this team uniquely qualified to execute?"

> "Three complementary capabilities:
>
> **Charles** has delivered technology transformation under pressure—LA Clippers, United Talent Agency. He knows how to ship complex projects with multiple stakeholders.
>
> **Keith** brings Stanford ecosystem connections through mediaX, plus policy experience from UN SDG Fund work. He knows how education and workforce development actually function at systemic levels.
>
> **Mike** has built and scaled platforms—Lyfe AI, Defy Mortgage. He knows the technical architecture and compliance requirements for education technology.
>
> We're not three people with similar backgrounds hoping to figure it out. We're **three distinct capabilities** that this project specifically requires."

### "Have you shipped something like this before?"

> "Not this exact product—but each of us has shipped in our domains.
>
> Charles delivered platform launches at organizations with massive scale and intense scrutiny. Keith built collaborative programs spanning multiple institutions and countries. Mike built compliant technology platforms in regulated industries.
>
> What we haven't done is combine these capabilities for education workforce development. That's why this team exists—each of us brings proven execution in the components this project requires."

---

## Risk & Contingency Questions

### "What if the AI gives students wrong information?"

> "That's a **feature of our pedagogy**, not a bug.
>
> We explicitly teach AI discernment—students learn to verify, cross-reference, and apply judgment. When the AI is wrong, that's a learning opportunity about critical evaluation.
>
> The Socratic model means the AI asks questions more than it provides answers. Students develop their own reasoning; the AI guides the process.
>
> And employer reviewers provide the ultimate verification. If a student's portfolio contains AI-generated nonsense, it won't pass employer evaluation. The system has multiple checkpoints."

### "What if employers don't actually hire from the program?"

> "We've built employer commitment into the design from day one.
>
> The 12 employer partners aren't just advisors—they're committing to review portfolios and offer micro-internships. They have **skin in the game** because they need talent and want to shape the pipeline.
>
> If placement rates fall short, we adjust. Maybe portfolio standards need recalibration. Maybe employer expectations aren't clear. The feedback loop is tight enough to iterate quickly.
>
> But the fundamental demand is real—employers are struggling to find entry-level candidates with demonstrated AI collaboration skills. We're building supply for existing demand."

### "When should you refrain from implementing AI?"

> "When it would replace human judgment that matters.
>
> AI should not evaluate soft skills, interpersonal dynamics, or ethical reasoning—humans do that. AI should not make final hiring decisions—employers do that. AI should not determine which students get opportunities—program design ensures equity.
>
> We use AI for **scaffolding and scale**—personalized guidance, pattern recognition, content generation. We use humans for **evaluation and relationships**—portfolio review, mentorship, employment decisions.
>
> The line is clear: AI amplifies human capability; it doesn't replace human judgment."

---

## Quick Reference: One-Line Responses

| Question Type | Key Message |
|--------------|-------------|
| Equity | "By design, not hope—explicit targets we track and adjust" |
| AI Limitations | "Students learn to verify, not just accept—that's the curriculum" |
| Evidence | "Research-backed approach with rigorous measurement plan" |
| Teacher Impact | "AI handles scaffolding; teachers focus on relationships" |
| Sustainability | "Three revenue streams: district CTE, employer sponsors, platform licensing" |
| Differentiation | "Portfolio-based outcomes, not credential accumulation" |
| Team | "Three distinct capabilities this project specifically requires" |

---

*Document prepared: February 4, 2026*
